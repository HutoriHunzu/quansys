{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Working with DataHandler and Aggregator\n",
    "\n",
    "This notebook demonstrates how to use the refactored **DataHandler** and **Aggregator** classes.\n",
    "\n",
    "## Key Steps\n",
    "1. **Set Up**: Import the necessary modules and classes.\n",
    "2. **Initialize a DataHandler**: Choose a root directory and create the folder structure.\n",
    "3. **Create Iterations**: For each iteration, register simulation outputs (identifiers) and save data.\n",
    "4. **Aggregation**: Use the `Aggregator` (invoked via `DataHandler.aggregate_and_save()`) to combine results.\n",
    "\n",
    "---\n",
    "**Important**: Make sure the files `data_handler.py`, `aggregator.py`, `metadata.py`, and their dependencies are accessible from the same environment or folder structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If your modules are in a local directory, adjust the Python path as needed.\n",
    "# For example:\n",
    "# import sys\n",
    "# sys.path.append('../src')\n",
    "\n",
    "from data_handler import DataHandler  # <-- Adjust if needed\n",
    "from aggregator import Aggregator      # <-- Adjust if needed\n",
    "\n",
    "# For demonstration, let's define a root directory inside a temporary folder.\n",
    "root_dir = Path(\"./demo_data_handler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Configure `DataHandler`\n",
    "We'll instantiate the `DataHandler` with our chosen root directory and some grouping configuration for aggregation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grouping configuration.\n",
    "# For demonstration, suppose we have two categories of results:\n",
    "# 1) 'metrics' that merges output from 'accuracy' and 'loss'.\n",
    "# 2) 'hyperparams' that merges data from 'params'.\n",
    "\n",
    "group_config = {\n",
    "    \"metrics\": [\"accuracy\", \"loss\"],\n",
    "    \"hyperparams\": [\"params\"]\n",
    "}\n",
    "\n",
    "# Initialize the DataHandler with the root directory and grouping config.\n",
    "data_handler = DataHandler(\n",
    "    root_directory=root_dir,\n",
    "    grouping_config=group_config\n",
    ")\n",
    "\n",
    "# Create the folder structure (with overwrite=True if you want a clean start)\n",
    "data_handler.create_folders(overwrite=True)\n",
    "\n",
    "print(f\"DataHandler initialized.\\nRoot: {data_handler.root_directory}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Iterations and Register Simulation Outputs\n",
    "Each iteration folder (e.g., `iteration_0`, `iteration_1`) contains a `metadata.json` that tracks what outputs have been created and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Create a new iteration\n",
    "iteration_folder_1 = data_handler.create_iteration()\n",
    "print(\"Created iteration folder:\", iteration_folder_1)\n",
    "\n",
    "# 3.2: Register identifiers for this iteration\n",
    "id_accuracy = data_handler.register_identifier(\"accuracy\")\n",
    "id_loss = data_handler.register_identifier(\"loss\")\n",
    "id_params = data_handler.register_identifier(\"params\")\n",
    "\n",
    "print(f\"Registered 'accuracy' --> {id_accuracy}\")\n",
    "print(f\"Registered 'loss'     --> {id_loss}\")\n",
    "print(f\"Registered 'params'   --> {id_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add JSON Data to Iteration\n",
    "Once an identifier is registered, we can add (save) the corresponding data to the iteration folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: Prepare some dummy JSON-serializable data\n",
    "accuracy_data = {\"epoch\": 1, \"accuracy\": 0.85}\n",
    "loss_data = {\"epoch\": 1, \"loss\": 0.45}\n",
    "params_data = {\"learning_rate\": 0.001, \"batch_size\": 32}\n",
    "\n",
    "# 4.2: Add them to the iteration\n",
    "data_handler.add_data_to_iteration(\"accuracy\", accuracy_data)\n",
    "data_handler.add_data_to_iteration(\"loss\", loss_data)\n",
    "data_handler.add_data_to_iteration(\"params\", params_data)\n",
    "\n",
    "print(\"Data saved and metadata updated for iteration_0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Create Another Iteration\n",
    "For demonstration, let's add a second iteration with slightly different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second iteration\n",
    "iteration_folder_2 = data_handler.create_new_iteration()\n",
    "print(\"Created iteration folder:\", iteration_folder_2)\n",
    "\n",
    "# Register the same identifiers\n",
    "data_handler.register_identifier(\"accuracy\")\n",
    "data_handler.register_identifier(\"loss\")\n",
    "data_handler.register_identifier(\"params\")\n",
    "\n",
    "# Save some different data.\n",
    "data_handler.add_data_to_iteration(\"accuracy\", {\"epoch\": 2, \"accuracy\": 0.90})\n",
    "data_handler.add_data_to_iteration(\"loss\", {\"epoch\": 2, \"loss\": 0.40})\n",
    "data_handler.add_data_to_iteration(\"params\", {\"learning_rate\": 0.0005, \"batch_size\": 64})\n",
    "\n",
    "print(\"Data saved and metadata updated for iteration_1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate and Save\n",
    "Finally, we'll invoke the DataHandler's aggregation method, which uses the `Aggregator` internally to scan all iterations, flatten the JSON outputs, and then save the results as CSV files under the `aggregations` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, this uses the 'analysis' adapter\n",
    "# If you want a custom adapter, provide a TypeAdapter instance.\n",
    "\n",
    "data_handler.aggregate_and_save()\n",
    "print(\"Aggregation completed. CSV files are in:\", data_handler.aggregations_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Aggregation Results\n",
    "The `DataHandler` wrote one CSV file per group (based on `group_config`) into the `aggregations` folder.\n",
    "\n",
    "- `metrics.csv` should contain merged data from `accuracy` and `loss`.\n",
    "- `hyperparams.csv` should contain data from `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the metrics.csv\n",
    "metrics_csv = data_handler.aggregations_directory / \"metrics.csv\"\n",
    "metrics_df = pd.read_csv(metrics_csv)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the hyperparams.csv\n",
    "hyperparams_csv = data_handler.aggregations_directory / \"hyperparams.csv\"\n",
    "hyperparams_df = pd.read_csv(hyperparams_csv)\n",
    "hyperparams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "We have:\n",
    "1. Created a **DataHandler** pointing to our `demo_data_handler` root.\n",
    "2. Generated two iterations of data, each with three identifiers (`accuracy`, `loss`, `params`).\n",
    "3. Saved the data in JSON form under each iteration folder, updating `metadata.json`.\n",
    "4. Aggregated the results according to `group_config`, producing CSV files for each group.\n",
    "\n",
    "Feel free to explore the folders in your file browser to see how everything is structured.\n",
    "You can also experiment with custom TypeAdapters, advanced flattening, or multi-layer dictionaries.\n",
    "\n",
    "Thank you for using the **DataHandler** and **Aggregator** tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
