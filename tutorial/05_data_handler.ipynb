{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Working with DataHandler and Aggregator\n",
    "\n",
    "This notebook demonstrates how to use the refactored **DataHandler** and **Aggregator** classes.\n",
    "\n",
    "## Key Steps\n",
    "1. **Set Up**: Import the necessary modules and classes.\n",
    "2. **Initialize a DataHandler**: Choose a root directory and create the folder structure.\n",
    "3. **Create Iterations**: For each iteration, register simulation outputs (identifiers) and save data.\n",
    "4. **Aggregation**: Use the `Aggregator` (invoked via `DataHandler.aggregate_and_save()`) to combine results.\n",
    "\n",
    "---\n",
    "**Important**: Make sure the files `data_handler.py`, `aggregator.py`, `metadata.py`, and their dependencies are accessible from the same environment or folder structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysubmit.workflow.data_handler import DataHandler  # <-- Adjust if needed\n",
    "\n",
    "# For demonstration, let's define a root directory inside a temporary folder.\n",
    "root_dir = Path(\"./demo_data_handler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Configure `DataHandler`\n",
    "We'll instantiate the `DataHandler` with our chosen root directory and some grouping configuration for aggregation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized.\n",
      "Root: demo_data_handler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a grouping configuration.\n",
    "# For demonstration, suppose we have two categories of results:\n",
    "# 1) 'metrics' that merges output from 'accuracy' and 'loss'.\n",
    "# 2) 'hyperparams' that merges data from 'params'.\n",
    "\n",
    "group_config = {\n",
    "    \"metrics\": [\"accuracy\", \"loss\"],\n",
    "    \"hyperparams\": [\"params\"]\n",
    "}\n",
    "\n",
    "# Initialize the DataHandler with the root directory and grouping config.\n",
    "data_handler = DataHandler(\n",
    "    root_directory=root_dir,\n",
    "    grouping_config=group_config\n",
    ")\n",
    "\n",
    "# Create the folder structure\n",
    "data_handler.create_folders()\n",
    "\n",
    "print(f\"DataHandler initialized.\\nRoot: {data_handler.root_directory}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Iterations and Register Simulation Outputs\n",
    "Each iteration folder (e.g., `iteration_0`, `iteration_1`) contains a `metadata.json` that tracks what outputs have been created and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fb11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created iteration folder: demo_data_handler\\results\\iterations\\iteration_0\n",
      "Registered 'accuracy' --> accuracy.json\n",
      "Registered 'loss'     --> loss.json\n",
      "Registered 'params'   --> params.json\n"
     ]
    }
   ],
   "source": [
    "# 3.1: Create a new iteration\n",
    "iteration_folder_1 = data_handler.create_iteration()\n",
    "print(\"Created iteration folder:\", iteration_folder_1)\n",
    "\n",
    "# 3.2: Register identifiers for this iteration\n",
    "id_accuracy = data_handler.register_identifier(\"accuracy\")\n",
    "id_loss = data_handler.register_identifier(\"loss\")\n",
    "id_params = data_handler.register_identifier(\"params\")\n",
    "\n",
    "print(f\"Registered 'accuracy' --> {id_accuracy}\")\n",
    "print(f\"Registered 'loss'     --> {id_loss}\")\n",
    "print(f\"Registered 'params'   --> {id_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add JSON Data to Iteration\n",
    "Once an identifier is registered, we can add (save) the corresponding data to the iteration folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved and metadata updated for iteration_0.\n"
     ]
    }
   ],
   "source": [
    "# 4.1: Prepare some dummy JSON-serializable data\n",
    "accuracy_data = {\"epoch\": 1, \"accuracy\": 0.85}\n",
    "loss_data = {\"epoch\": 1, \"loss\": 0.45}\n",
    "params_data = {\"learning_rate\": 0.001, \"batch_size\": 32}\n",
    "\n",
    "# 4.2: Add them to the iteration\n",
    "data_handler.add_data_to_iteration(\"accuracy\", accuracy_data)\n",
    "data_handler.add_data_to_iteration(\"loss\", loss_data)\n",
    "data_handler.add_data_to_iteration(\"params\", params_data)\n",
    "\n",
    "print(\"Data saved and metadata updated for iteration_0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Create Another Iteration\n",
    "For demonstration, let's add a second iteration with slightly different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataHandler' object has no attribute 'create_new_iteration'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a second iteration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m iteration_folder_2 = \u001b[43mdata_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_new_iteration\u001b[49m()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated iteration folder:\u001b[39m\u001b[33m\"\u001b[39m, iteration_folder_2)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Register the same identifiers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosengrp\\Documents\\GitHub\\pysubmit\\.venv\\Lib\\site-packages\\pydantic\\main.py:856\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataHandler' object has no attribute 'create_new_iteration'"
     ]
    }
   ],
   "source": [
    "# Create a second iteration\n",
    "iteration_folder_2 = data_handler.create_iteration()\n",
    "print(\"Created iteration folder:\", iteration_folder_2)\n",
    "\n",
    "# Register the same identifiers\n",
    "data_handler.register_identifier(\"accuracy\")\n",
    "data_handler.register_identifier(\"loss\")\n",
    "data_handler.register_identifier(\"params\")\n",
    "\n",
    "# Save some different data.\n",
    "data_handler.add_data_to_iteration(\"accuracy\", {\"epoch\": 2, \"accuracy\": 0.90})\n",
    "data_handler.add_data_to_iteration(\"loss\", {\"epoch\": 2, \"loss\": 0.40})\n",
    "data_handler.add_data_to_iteration(\"params\", {\"learning_rate\": 0.0005, \"batch_size\": 64})\n",
    "\n",
    "print(\"Data saved and metadata updated for iteration_1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate and Save\n",
    "Finally, we'll invoke the DataHandler's aggregation method, which uses the `Aggregator` internally to scan all iterations, flatten the JSON outputs, and then save the results as CSV files under the `aggregations` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, this uses the 'analysis' adapter\n",
    "# If you want a custom adapter, provide a TypeAdapter instance.\n",
    "\n",
    "data_handler.aggregate_and_save()\n",
    "print(\"Aggregation completed. CSV files are in:\", data_handler.aggregations_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Aggregation Results\n",
    "The `DataHandler` wrote one CSV file per group (based on `group_config`) into the `aggregations` folder.\n",
    "\n",
    "- `metrics.csv` should contain merged data from `accuracy` and `loss`.\n",
    "- `hyperparams.csv` should contain data from `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the metrics.csv\n",
    "metrics_csv = data_handler.aggregations_directory / \"metrics.csv\"\n",
    "metrics_df = pd.read_csv(metrics_csv)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the hyperparams.csv\n",
    "hyperparams_csv = data_handler.aggregations_directory / \"hyperparams.csv\"\n",
    "hyperparams_df = pd.read_csv(hyperparams_csv)\n",
    "hyperparams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "We have:\n",
    "1. Created a **DataHandler** pointing to our `demo_data_handler` root.\n",
    "2. Generated two iterations of data, each with three identifiers (`accuracy`, `loss`, `params`).\n",
    "3. Saved the data in JSON form under each iteration folder, updating `metadata.json`.\n",
    "4. Aggregated the results according to `group_config`, producing CSV files for each group.\n",
    "\n",
    "Feel free to explore the folders in your file browser to see how everything is structured.\n",
    "You can also experiment with custom TypeAdapters, advanced flattening, or multi-layer dictionaries.\n",
    "\n",
    "Thank you for using the **DataHandler** and **Aggregator** tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
